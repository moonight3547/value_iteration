{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ce80ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "import warnings\n",
    "\n",
    "if not hasattr(np, 'bool8'):\n",
    "    np.bool8 = np.bool_\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from value_iteration import *\n",
    "from envs.tictactoe.tictactoe_gui_env import TicTacToeGUIEnv\n",
    "from envs.tictactoe.tictactoe_2p_env import TicTacToeEnv2\n",
    "\n",
    "#env_two_agents = TicTacToeEnv2(size = 3)\n",
    "#print(env_two_agents.reward[2450][4])\n",
    "#print(env_two_agents.state_type[10920])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ebf29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First player against random player:\n",
      "Converged in 4 Iterations\n",
      "Second player against random player:\n",
      "Converged in 4 Iterations\n"
     ]
    }
   ],
   "source": [
    "env1 = gym.make('TicTacToeGUI-v0',size = 3,player = 1)\n",
    "env2 = gym.make('TicTacToeGUI-v0',size = 3,player = 2)\n",
    "VI1 = ValueIterationAgent(env1, gamma=1, iters=10000, eval_iters=100, eps=1e-5, seed=233333)\n",
    "print(\"First player against random player:\")\n",
    "VI1.value_iteration()\n",
    "VI1.get_policy()\n",
    "VI2 = ValueIterationAgent(env2, gamma=1, iters=10000, eval_iters=100, eps=1e-5, seed=233333)\n",
    "print(\"Second player against random player:\")\n",
    "VI2.value_iteration()\n",
    "VI2.get_policy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb6c2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1: player1 value delta 229.921875, player2 value delta: 511.3714285714285\n",
      "iteration 2: player1 value delta 117.625, player2 value delta: 143.6\n",
      "iteration 3: player1 value delta 18.0, player2 value delta: 5.333333333333333\n",
      "iteration 4: player1 value delta 1.0, player2 value delta: 0.0\n",
      "iteration 5: player1 value delta 0.0, player2 value delta: 0.0\n",
      "Converged in 4 Iterations\n",
      "Step 1, state 0 Player 1 moves (0, 0) reward (np.float64(0.0), np.float64(0.0)): \n",
      "X . .\n",
      ". . .\n",
      ". . .\n",
      "Step 2, state 6561 Player 2 moves (1, 1) reward (np.float64(0.0), np.float64(0.0)): \n",
      "X . .\n",
      ". O .\n",
      ". . .\n",
      "Step 3, state 6723 Player 1 moves (0, 1) reward (np.float64(0.0), np.float64(0.0)): \n",
      "X X .\n",
      ". O .\n",
      ". . .\n",
      "Step 4, state 8910 Player 2 moves (0, 2) reward (np.float64(0.0), np.float64(0.0)): \n",
      "X X O\n",
      ". O .\n",
      ". . .\n",
      "Step 5, state 10368 Player 1 moves (2, 0) reward (np.float64(0.0), np.float64(0.0)): \n",
      "X X O\n",
      ". O .\n",
      "X . .\n",
      "Step 6, state 10377 Player 2 moves (1, 0) reward (np.float64(0.0), np.float64(0.0)): \n",
      "X X O\n",
      "O O .\n",
      "X . .\n",
      "Step 7, state 10863 Player 1 moves (1, 2) reward (np.float64(0.0), np.float64(0.0)): \n",
      "X X O\n",
      "O O X\n",
      "X . .\n",
      "Step 8, state 10890 Player 2 moves (2, 1) reward (np.float64(0.0), np.float64(0.0)): \n",
      "X X O\n",
      "O O X\n",
      "X O .\n",
      "Step 9, state 10896 Player 1 moves (2, 2) reward (np.float64(0.0), np.float64(0.0)): \n",
      "X X O\n",
      "O O X\n",
      "X O X\n"
     ]
    }
   ],
   "source": [
    "from envs.tictactoe.tictactoe_2p_env import two_agents, run_NE\n",
    "env_new = TicTacToeEnv2(size = 3)\n",
    "V1, V2, pi1, pi2 = two_agents(env_new,VI1,VI2, gamma = 1, iters = 100)\n",
    "run_NE(env_new, VI1, VI2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd40fc8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(min(VI1.value), min(VI2.value), max(VI1.value), max(VI2.value))\\nprint(np.argmin(VI2.value))\\ns0=10817\\nprint(env_new.id_to_state[s0])\\ns0 = 12342 #10863\\nprint(env_new.reward[s0], V1[s0])\\nactions = [6, 8] #[5, 7, 8]\\nstates = [env_new.get_state(1, s0, a) for a in actions]\\nfor s in states:\\n    print(env_new.id_to_state[s])\\n    print(V2[s])\\n    s2 = env_new.get_state(2, s, 5)\\n    print(env_new.id_to_state[s2])\\n    print(s2, V1[s2], env_new.state_type[s2])\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(min(VI1.value), min(VI2.value), max(VI1.value), max(VI2.value))\n",
    "print(np.argmin(VI2.value))\n",
    "s0=10817\n",
    "print(env_new.id_to_state[s0])\n",
    "s0 = 12342 #10863\n",
    "print(env_new.reward[s0], V1[s0])\n",
    "actions = [6, 8] #[5, 7, 8]\n",
    "states = [env_new.get_state(1, s0, a) for a in actions]\n",
    "for s in states:\n",
    "    print(env_new.id_to_state[s])\n",
    "    print(V2[s])\n",
    "    s2 = env_new.get_state(2, s, 5)\n",
    "    print(env_new.id_to_state[s2])\n",
    "    print(s2, V1[s2], env_new.state_type[s2])\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dda4230rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
